{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Analysis\n",
    "\n",
    "Consolidated analysis notebook for distributive justice experiments.\n",
    "\n",
    "**Key Features:**\n",
    "- Load results from simplified experiment files\n",
    "- Statistical analysis of principle preferences\n",
    "- Visualization of consensus patterns\n",
    "- Export results for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Analysis environment initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_results(experiment_ids: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load experiment results from JSON files.\n",
    "    \n",
    "    Args:\n",
    "        experiment_ids: List of experiment IDs to load\n",
    "    \n",
    "    Returns:\n",
    "        List of experiment result dictionaries\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    results_dir = Path(\"experiment_results\")\n",
    "    \n",
    "    for exp_id in experiment_ids:\n",
    "        # Try different file naming patterns\n",
    "        possible_files = [\n",
    "            results_dir / f\"{exp_id}.json\",\n",
    "            results_dir / f\"{exp_id}_complete.json\"\n",
    "        ]\n",
    "        \n",
    "        for file_path in possible_files:\n",
    "            if file_path.exists():\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                    results.append(data)\n",
    "                    print(f\"‚úÖ Loaded: {file_path}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Not found: {exp_id}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load recent experiment results\n",
    "experiment_ids = [\"real_test\", \"batch_test_001\", \"batch_test_002\"]\n",
    "experiments = load_experiment_results(experiment_ids)\n",
    "\n",
    "print(f\"\\nüìä Loaded {len(experiments)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_experiment_summary(experiment_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract key summary information from experiment data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metadata = experiment_data.get('experiment_metadata', {})\n",
    "        \n",
    "        summary = {\n",
    "            'experiment_id': metadata.get('experiment_id', 'unknown'),\n",
    "            'num_agents': metadata.get('num_agents', 0),\n",
    "            'max_rounds': metadata.get('max_rounds', 0),\n",
    "            'consensus_reached': metadata.get('consensus_reached', False),\n",
    "            'total_duration': metadata.get('total_duration_seconds', 0),\n",
    "            'rounds_to_consensus': metadata.get('rounds_to_consensus', 0),\n",
    "            'agreed_principle': metadata.get('agreed_principle', {}),\n",
    "            'total_messages': len(experiment_data.get('deliberation_transcript', []))\n",
    "        }\n",
    "        \n",
    "        # Extract agreed principle name\n",
    "        if summary['agreed_principle']:\n",
    "            summary['agreed_principle_name'] = summary['agreed_principle'].get('principle_name', 'Unknown')\n",
    "            summary['agreed_principle_id'] = summary['agreed_principle'].get('principle_id', 0)\n",
    "        else:\n",
    "            summary['agreed_principle_name'] = None\n",
    "            summary['agreed_principle_id'] = None\n",
    "        \n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting summary: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Process all experiments\n",
    "if experiments:\n",
    "    experiment_summaries = []\n",
    "    \n",
    "    for exp_data in experiments:\n",
    "        summary = extract_experiment_summary(exp_data)\n",
    "        if summary:\n",
    "            experiment_summaries.append(summary)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(experiment_summaries)\n",
    "    \n",
    "    print(f\"üìä Processed {len(df)} experiments\")\n",
    "    print(\"\\nExperiment Summary:\")\n",
    "    print(df[['experiment_id', 'consensus_reached', 'total_duration', 'agreed_principle_name']])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No experiments loaded\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"üìä EXPERIMENT STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"Total experiments: {len(df)}\")\n",
    "    print(f\"Consensus reached: {df['consensus_reached'].sum()} ({df['consensus_reached'].mean():.1%})\")\n",
    "    print(f\"Average duration: {df['total_duration'].mean():.1f} seconds\")\n",
    "    print(f\"Average rounds to consensus: {df['rounds_to_consensus'].mean():.1f}\")\n",
    "    print(f\"Average messages: {df['total_messages'].mean():.1f}\")\n",
    "    \n",
    "    # Principle preferences\n",
    "    if df['agreed_principle_name'].notna().any():\n",
    "        print(\"\\nüéØ Principle Preferences:\")\n",
    "        principle_counts = df[df['consensus_reached']]['agreed_principle_name'].value_counts()\n",
    "        for principle, count in principle_counts.items():\n",
    "            print(f\"   {principle}: {count} experiments\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    print(\"\\n‚è±Ô∏è  Performance Metrics:\")\n",
    "    print(f\"   Fastest experiment: {df['total_duration'].min():.1f}s\")\n",
    "    print(f\"   Slowest experiment: {df['total_duration'].max():.1f}s\")\n",
    "    print(f\"   Median duration: {df['total_duration'].median():.1f}s\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Experiment Analysis Dashboard', fontsize=16)\n",
    "    \n",
    "    # 1. Consensus rate\n",
    "    consensus_counts = df['consensus_reached'].value_counts()\n",
    "    axes[0, 0].pie(consensus_counts.values, labels=['Consensus' if x else 'No Consensus' for x in consensus_counts.index], \n",
    "                   autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 0].set_title('Consensus Rate')\n",
    "    \n",
    "    # 2. Duration distribution\n",
    "    axes[0, 1].hist(df['total_duration'], bins=max(3, len(df)//2), alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Duration (seconds)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Duration Distribution')\n",
    "    \n",
    "    # 3. Principle preferences\n",
    "    if df['agreed_principle_name'].notna().any():\n",
    "        principle_counts = df[df['consensus_reached']]['agreed_principle_name'].value_counts()\n",
    "        axes[1, 0].bar(range(len(principle_counts)), principle_counts.values)\n",
    "        axes[1, 0].set_xticks(range(len(principle_counts)))\n",
    "        axes[1, 0].set_xticklabels([name[:20] + '...' if len(name) > 20 else name for name in principle_counts.index], \n",
    "                                  rotation=45, ha='right')\n",
    "        axes[1, 0].set_ylabel('Count')\n",
    "        axes[1, 0].set_title('Agreed Principles')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No consensus data available', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].set_title('Agreed Principles')\n",
    "    \n",
    "    # 4. Rounds to consensus\n",
    "    if df['rounds_to_consensus'].sum() > 0:\n",
    "        consensus_df = df[df['consensus_reached']]\n",
    "        if len(consensus_df) > 0:\n",
    "            axes[1, 1].bar(consensus_df['experiment_id'], consensus_df['rounds_to_consensus'])\n",
    "            axes[1, 1].set_xlabel('Experiment')\n",
    "            axes[1, 1].set_ylabel('Rounds to Consensus')\n",
    "            axes[1, 1].set_title('Rounds to Consensus by Experiment')\n",
    "            axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'No consensus reached', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "            axes[1, 1].set_title('Rounds to Consensus')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No rounds data available', ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Rounds to Consensus')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Export analysis results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Export summary data\n",
    "    export_file = f\"analysis_results_{timestamp}.csv\"\n",
    "    df.to_csv(export_file, index=False)\n",
    "    print(f\"üìä Summary data exported to: {export_file}\")\n",
    "    \n",
    "    # Export analysis summary\n",
    "    analysis_summary = {\n",
    "        'timestamp': timestamp,\n",
    "        'total_experiments': len(df),\n",
    "        'consensus_rate': df['consensus_reached'].mean(),\n",
    "        'average_duration': df['total_duration'].mean(),\n",
    "        'average_rounds': df['rounds_to_consensus'].mean(),\n",
    "        'principle_preferences': df[df['consensus_reached']]['agreed_principle_name'].value_counts().to_dict() if df['agreed_principle_name'].notna().any() else {}\n",
    "    }\n",
    "    \n",
    "    summary_file = f\"analysis_summary_{timestamp}.json\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(analysis_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"üìä Analysis summary exported to: {summary_file}\")\n",
    "    \n",
    "    # Display final summary\n",
    "    print(\"\\nüéØ FINAL ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Experiments analyzed: {len(df)}\")\n",
    "    print(f\"Consensus rate: {df['consensus_reached'].mean():.1%}\")\n",
    "    print(f\"Average duration: {df['total_duration'].mean():.1f} seconds\")\n",
    "    print(f\"Most common principle: {df[df['consensus_reached']]['agreed_principle_name'].mode().iloc[0] if df['consensus_reached'].any() and df['agreed_principle_name'].notna().any() else 'None'}\")\n",
    "    print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run New Experiments\n",
    "\n",
    "Use the simplified experiment system to run new experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: Generate and run new experiments\nfrom config_generator import create_test_generator\nfrom run_experiment import run_experiment_sync\nfrom run_batch import run_batch_sync\n\n# Create a new test configuration\nprint(\"üîß Creating new test configuration...\")\ngenerator = create_test_generator()\nconfig_path = generator.generate_and_save_config(\"notebook_test.yaml\", \"notebook_test\")\nprint(f\"‚úÖ Created: {config_path}\")\n\n# Uncomment the following lines to run a new experiment:\n# print(\"\\nüöÄ Running new experiment...\")\n# result = run_experiment_sync(\"notebook_test\")\n# if result[\"success\"]:\n#     print(f\"‚úÖ Experiment completed: {result['experiment_id']}\")\n#     print(f\"   Consensus: {result['consensus_reached']}\")\n#     print(f\"   Duration: {result['duration_seconds']:.1f}s\")\n# else:\n#     print(f\"‚ùå Experiment failed: {result['error']}\")\n\nprint(\"\\nüí° To run experiments, uncomment the code above or use the command line:\")\nprint(\"   python run_experiment.py notebook_test\")\nprint(\"   python run_batch.py config1 config2 --max-concurrent 2\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This consolidated notebook provides:\n",
    "\n",
    "1. **Data Loading**: Load experiment results from simplified JSON files\n",
    "2. **Processing**: Extract key metrics and summaries\n",
    "3. **Statistics**: Calculate basic statistics and performance metrics\n",
    "4. **Visualization**: Create comprehensive analysis dashboard\n",
    "5. **Export**: Save results for further analysis\n",
    "6. **Integration**: Interface with simplified experiment system\n",
    "\n",
    "**Usage:**\n",
    "- Run cells sequentially to analyze existing experiments\n",
    "- Modify `experiment_ids` list to analyze different experiments\n",
    "- Use the simplified Python files for new experiments\n",
    "- Export results for external analysis tools\n",
    "\n",
    "**Next Steps:**\n",
    "- Run more experiments using `run_experiment.py` and `run_batch.py`\n",
    "- Add more sophisticated statistical analysis as needed\n",
    "- Create custom visualizations for specific research questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}