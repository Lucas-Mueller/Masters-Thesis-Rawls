{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematic Multi-Agent Distributive Justice Analysis\n",
    "\n",
    "This notebook performs comprehensive analysis of distributive justice principles across different agent configurations.\n",
    "\n",
    "## Experiment Design\n",
    "- **Models**: \"gpt-4.1\", \"gpt-4.1-mini\", \"gpt-4.1-nano\" \n",
    "- **Temperatures**: 0, 1, 2\n",
    "- **Prompts**: US, Canadian, Polish college students  \n",
    "- **Income Distributions**: 4 distributions from new_logic.md\n",
    "- **Total Configurations**: 3 × 3 × 3 × 4 = **108 unique combinations**\n",
    "\n",
    "## Configuration Generation Method\n",
    "- **SYSTEMATIC**: Uses `itertools.product()` to generate each unique combination exactly once\n",
    "- **NOT RANDOM**: Completely deterministic, exhaustive coverage\n",
    "\n",
    "## Analysis Sections\n",
    "1. Configuration Setup & Generation\n",
    "2. Parallel Batch Execution  \n",
    "3. Results Analysis & Visualization\n",
    "4. Parameter Impact Analysis\n",
    "5. Satisfaction Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MAAI modules imported successfully\n",
      "🎯 Systematic Analysis Notebook Ready\n",
      "📊 Configuration: All models included (gpt-4.1, gpt-4.1-mini, gpt-4.1-nano)\n"
     ]
    }
   ],
   "source": [
    "# Setup paths and imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "import nest_asyncio\n",
    "\n",
    "# Data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Add MAAI to path\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "# Import MAAI modules\n",
    "try:\n",
    "    from maai.runners import run_batch\n",
    "    from maai.config.manager import load_config_from_file\n",
    "    from maai.core.models import IncomeDistribution, IncomeClass\n",
    "    print(\"✅ MAAI modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing MAAI modules: {e}\")\n",
    "    print(\"💡 Make sure you're running from the correct directory\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"🎯 Systematic Analysis Notebook Ready\")\n",
    "print(\"📊 Configuration: All models included (gpt-4.1, gpt-4.1-mini, gpt-4.1-nano)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment parameters\n",
    "MODELS = [\"gpt-4.1-mini\"]\n",
    "TEMPERATURES = [0, 1, 2]  # Removed 0.5 for simplicity\n",
    "PROMPTS = {\n",
    "    \"us_student\": \"You are a US college student participating in an economic experiment about distributive justice principles. You will receive real monetary payouts based on your income class assignment after the group chooses a principle.\",\n",
    "    \"canadian_student\": \"You are a Canadian college student participating in an economic experiment about distributive justice principles. You will receive real monetary payouts based on your income class assignment after the group chooses a principle.\",\n",
    "    \"polish_student\": \"You are a Polish college student participating in an economic experiment about distributive justice principles. You will receive real monetary payouts based on your income class assignment after the group chooses a principle.\"\n",
    "}\n",
    "\n",
    "# Income distributions from new_logic.md specification\n",
    "# IMPORTANT: Keys must match IncomeClass enum values exactly\n",
    "INCOME_DISTRIBUTIONS = [\n",
    "    {\n",
    "        'distribution_id': 1,\n",
    "        'name': 'Distribution 1',\n",
    "        'income_by_class': {\n",
    "            'High': 32000,           # IncomeClass.HIGH = \"High\"\n",
    "            'Medium high': 27000,    # IncomeClass.MEDIUM_HIGH = \"Medium high\"\n",
    "            'Medium': 24000,         # IncomeClass.MEDIUM = \"Medium\"\n",
    "            'Medium low': 13000,     # IncomeClass.MEDIUM_LOW = \"Medium low\"\n",
    "            'Low': 12000             # IncomeClass.LOW = \"Low\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'distribution_id': 2,\n",
    "        'name': 'Distribution 2',\n",
    "        'income_by_class': {\n",
    "            'High': 28000,\n",
    "            'Medium high': 22000,\n",
    "            'Medium': 20000,\n",
    "            'Medium low': 17000,\n",
    "            'Low': 13000\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'distribution_id': 3,\n",
    "        'name': 'Distribution 3',\n",
    "        'income_by_class': {\n",
    "            'High': 31000,\n",
    "            'Medium high': 24000,\n",
    "            'Medium': 21000,\n",
    "            'Medium low': 16000,\n",
    "            'Low': 14000\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'distribution_id': 4,\n",
    "        'name': 'Distribution 4',\n",
    "        'income_by_class': {\n",
    "            'High': 21000,\n",
    "            'Medium high': 20000,\n",
    "            'Medium': 19000,\n",
    "            'Medium low': 16000,\n",
    "            'Low': 15000\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = len(MODELS) * len(TEMPERATURES) * len(PROMPTS)\n",
    "\n",
    "print(f\"📋 Experiment Parameters:\")\n",
    "print(f\"   Models: {len(MODELS)} ({', '.join(MODELS)})\")\n",
    "print(f\"   Temperatures: {len(TEMPERATURES)} ({TEMPERATURES})\")\n",
    "print(f\"   Prompts: {len(PROMPTS)} ({', '.join(PROMPTS.keys())})\")\n",
    "print(f\"   Distributions: {len(INCOME_DISTRIBUTIONS)}\")\n",
    "print(f\"   \")\n",
    "print(f\"🔢 Total Configurations: {total_combinations}\")\n",
    "print(f\"   Generation method: SYSTEMATIC (each unique combination created exactly once)\")\n",
    "print(f\"✅ Income class keys fixed to match IncomeClass enum values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_systematic_config(model: str, temperature: float, prompt_key: str, distribution_set: List[Dict], config_id: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Create a systematic experiment configuration.\n",
    "    \n",
    "    Args:\n",
    "        model: Model name (e.g., \"gpt-4.1-mini\")\n",
    "        temperature: Temperature value (0, 1, 2)\n",
    "        prompt_key: Prompt identifier (\"us_student\", \"canadian_student\", \"polish_student\")\n",
    "        distribution_set: List of income distributions\n",
    "        config_id: Unique configuration identifier\n",
    "    \n",
    "    Returns:\n",
    "        Configuration dictionary\n",
    "    \"\"\"\n",
    "    prompt_text = PROMPTS[prompt_key]\n",
    "    \n",
    "    config = {\n",
    "        'experiment_id': config_id,\n",
    "        'global_temperature': temperature,\n",
    "        \n",
    "        # Metadata for analysis\n",
    "        'systematic_analysis': {\n",
    "            'model': model,\n",
    "            'temperature': temperature,\n",
    "            'prompt_type': prompt_key,\n",
    "            'distribution_count': len(distribution_set)\n",
    "        },\n",
    "        \n",
    "        'experiment': {\n",
    "            'max_rounds': 6,  # Sufficient for consensus\n",
    "            'decision_rule': 'unanimity',\n",
    "            'timeout_seconds': 300\n",
    "        },\n",
    "        \n",
    "        # New game logic settings\n",
    "        'individual_rounds': 4,\n",
    "        'payout_ratio': 0.0001,\n",
    "        'enable_detailed_examples': True,\n",
    "        'enable_secret_ballot': True,\n",
    "        \n",
    "        # Income distribution scenarios\n",
    "        'income_distributions': distribution_set,\n",
    "        \n",
    "        # Memory strategy\n",
    "        'memory_strategy': 'decomposed',\n",
    "        \n",
    "        # Agent configuration - 3 agents with same prompt\n",
    "        'agents': [\n",
    "            {\n",
    "                'name': f'Agent_1_{prompt_key}',\n",
    "                'model': model,\n",
    "                'personality': prompt_text,\n",
    "                'temperature': temperature\n",
    "            },\n",
    "            {\n",
    "                'name': f'Agent_2_{prompt_key}',\n",
    "                'model': model,\n",
    "                'personality': prompt_text,\n",
    "                'temperature': temperature\n",
    "            },\n",
    "            {\n",
    "                'name': f'Agent_3_{prompt_key}',\n",
    "                'model': model,\n",
    "                'personality': prompt_text,\n",
    "                'temperature': temperature\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        'defaults': {\n",
    "            'personality': prompt_text,\n",
    "            'model': model,\n",
    "            'temperature': temperature\n",
    "        },\n",
    "        \n",
    "        'output': {\n",
    "            'directory': 'systematic_analysis_results',\n",
    "            'formats': ['json']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return config\n",
    "\n",
    "def generate_all_configurations() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate ALL unique experiment configurations systematically.\n",
    "    Uses itertools.product() to ensure each combination appears exactly once.\n",
    "    \n",
    "    Returns:\n",
    "        List of configuration dictionaries, one per unique combination\n",
    "    \"\"\"\n",
    "    configurations = []\n",
    "    config_index = 1\n",
    "    \n",
    "    # Create all combinations systematically - each appears exactly once\n",
    "    for model, temperature, prompt_key in product(MODELS, TEMPERATURES, PROMPTS.keys()):\n",
    "        config_id = f\"systematic_{config_index:03d}_{model.replace('-', '_')}_{temperature}_{prompt_key}\"\n",
    "        \n",
    "        config = create_systematic_config(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            prompt_key=prompt_key,\n",
    "            distribution_set=INCOME_DISTRIBUTIONS,\n",
    "            config_id=config_id\n",
    "        )\n",
    "        \n",
    "        configurations.append(config)\n",
    "        config_index += 1\n",
    "    \n",
    "    return configurations\n",
    "\n",
    "# Generate configurations systematically\n",
    "all_configs = generate_all_configurations()\n",
    "\n",
    "print(f\"✅ Generated {len(all_configs)} unique configurations systematically\")\n",
    "print(f\"📊 Sample configuration IDs:\")\n",
    "for i, config in enumerate(all_configs[:5]):\n",
    "    print(f\"   {i+1}. {config['experiment_id']}\")\n",
    "if len(all_configs) > 5:\n",
    "    print(f\"   ... and {len(all_configs) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "systematic_dir = Path(\"systematic_analysis\")\n",
    "config_dir = systematic_dir / \"configs\"\n",
    "results_dir = systematic_dir / \"results\"\n",
    "\n",
    "config_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save all configurations\n",
    "config_files = []\n",
    "for config in all_configs:\n",
    "    config_file = config_dir / f\"{config['experiment_id']}.yaml\"\n",
    "    \n",
    "    with open(config_file, 'w') as f:\n",
    "        yaml.dump(config, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    config_files.append(config['experiment_id'])\n",
    "\n",
    "print(f\"✅ Saved {len(config_files)} configuration files to {config_dir}\")\n",
    "print(f\"📁 Results will be saved to: {results_dir}\")\n",
    "\n",
    "# Display configuration summary\n",
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        'Config ID': config['experiment_id'],\n",
    "        'Model': config['systematic_analysis']['model'],\n",
    "        'Temperature': config['systematic_analysis']['temperature'],\n",
    "        'Prompt Type': config['systematic_analysis']['prompt_type'],\n",
    "        'Agents': len(config['agents']),\n",
    "        'Distributions': len(config['income_distributions'])\n",
    "    }\n",
    "    for config in all_configs\n",
    "])\n",
    "\n",
    "print(\"\\n📊 Configuration Summary:\")\n",
    "print(summary_df.head(10))\n",
    "print(f\"\\nTotal configurations: {len(summary_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parallel Batch Execution\n",
    "\n",
    "This section runs all configurations in parallel batches using the existing batch execution system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for batch execution\n",
    "MAX_CONCURRENT = 4  # Conservative for API limits\n",
    "BATCH_SIZE = 12     # Process in smaller batches for better monitoring\n",
    "\n",
    "print(f\"🚀 Starting systematic batch execution\")\n",
    "print(f\"   Total configurations: {len(config_files)}\")\n",
    "print(f\"   Max concurrent: {MAX_CONCURRENT}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Estimated batches: {(len(config_files) + BATCH_SIZE - 1) // BATCH_SIZE}\")\n",
    "\n",
    "# All configurations will be run (no test mode)\n",
    "config_files_to_run = config_files\n",
    "\n",
    "print(f\"\\n📋 Configurations to execute: {len(config_files_to_run)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute batch experiments\n",
    "import time\n",
    "\n",
    "async def run_systematic_experiments():\n",
    "    \"\"\"\n",
    "    Run all systematic experiments using the existing batch system.\n",
    "    \"\"\"\n",
    "    batch_start_time = time.time()\n",
    "    \n",
    "    # Run all experiments in parallel\n",
    "    results = await run_batch(\n",
    "        config_files_to_run,\n",
    "        max_concurrent=MAX_CONCURRENT,\n",
    "        output_dir=str(results_dir),\n",
    "        config_dir=str(config_dir)\n",
    "    )\n",
    "    \n",
    "    batch_duration = time.time() - batch_start_time\n",
    "    \n",
    "    # Analyze results\n",
    "    successful = [r for r in results if r.get('success', False)]\n",
    "    failed = [r for r in results if not r.get('success', False)]\n",
    "    \n",
    "    print(f\"\\n🎯 Systematic Execution Complete!\")\n",
    "    print(f\"   Total time: {batch_duration:.1f}s ({batch_duration/60:.1f} minutes)\")\n",
    "    print(f\"   Successful: {len(successful)}/{len(results)}\")\n",
    "    print(f\"   Failed: {len(failed)}/{len(results)}\")\n",
    "    print(f\"   Average per experiment: {batch_duration/len(results):.1f}s\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\n❌ Failed experiments:\")\n",
    "        for result in failed[:5]:  # Show first 5 failures\n",
    "            print(f\"   - {result.get('experiment_id', 'unknown')}: {result.get('error', 'unknown error')}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the experiments\n",
    "print(\"⏳ Starting batch execution...\")\n",
    "experiment_results = await run_systematic_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_data(results_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and process all experiment results into a structured DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        results_dir: Directory containing result JSON files\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with experiment results and metadata\n",
    "    \"\"\"\n",
    "    result_files = list(results_dir.glob(\"*.json\"))\n",
    "    \n",
    "    if not result_files:\n",
    "        print(\"❌ No result files found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"📊 Loading {len(result_files)} experiment results...\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for result_file in result_files:\n",
    "        try:\n",
    "            with open(result_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract experiment metadata\n",
    "            metadata = data.get('experiment_metadata', {})\n",
    "            consensus = metadata.get('final_consensus', {})\n",
    "            \n",
    "            # Parse config ID for systematic analysis metadata\n",
    "            experiment_id = metadata.get('experiment_id', result_file.stem)\n",
    "            \n",
    "            # Extract systematic analysis parameters from config ID\n",
    "            id_parts = experiment_id.split('_')\n",
    "            if len(id_parts) >= 5:\n",
    "                model = id_parts[2] + '-' + id_parts[3] + ('-' + id_parts[4] if len(id_parts) > 5 else '')\n",
    "                temperature = float(id_parts[-2])\n",
    "                prompt_type = id_parts[-1]\n",
    "            else:\n",
    "                model = 'unknown'\n",
    "                temperature = 0.0\n",
    "                prompt_type = 'unknown'\n",
    "            \n",
    "            # Extract agent data\n",
    "            agent_data = {k: v for k, v in data.items() if k != 'experiment_metadata'}\n",
    "            \n",
    "            # Process each agent's data\n",
    "            for agent_name, agent_info in agent_data.items():\n",
    "                agent_record = {\n",
    "                    'experiment_id': experiment_id,\n",
    "                    'agent_name': agent_name,\n",
    "                    'model': model,\n",
    "                    'temperature': temperature,\n",
    "                    'prompt_type': prompt_type,\n",
    "                    'consensus_reached': consensus.get('agreement_reached', False),\n",
    "                    'agreed_principle': consensus.get('agreed_principle', None),\n",
    "                    'total_duration': metadata.get('total_duration_seconds', 0),\n",
    "                    'final_principle_id': consensus.get('principle_choice', {}).get('principle_id', None),\n",
    "                }\n",
    "                \n",
    "                # Extract preference rankings\n",
    "                rankings = agent_info.get('preference_rankings', [])\n",
    "                if rankings:\n",
    "                    # Get initial, post-individual, and final rankings\n",
    "                    initial_rankings = [r for r in rankings if r.get('phase') == 'initial']\n",
    "                    post_individual_rankings = [r for r in rankings if r.get('phase') == 'post_individual']\n",
    "                    final_rankings = [r for r in rankings if r.get('phase') == 'final']\n",
    "                    \n",
    "                    if initial_rankings:\n",
    "                        agent_record['initial_ranking'] = initial_rankings[-1].get('rankings', [])\n",
    "                        agent_record['initial_certainty'] = initial_rankings[-1].get('certainty_level', 'unknown')\n",
    "                    \n",
    "                    if post_individual_rankings:\n",
    "                        agent_record['post_individual_ranking'] = post_individual_rankings[-1].get('rankings', [])\n",
    "                        agent_record['post_individual_certainty'] = post_individual_rankings[-1].get('certainty_level', 'unknown')\n",
    "                    \n",
    "                    if final_rankings:\n",
    "                        agent_record['final_ranking'] = final_rankings[-1].get('rankings', [])\n",
    "                        agent_record['final_certainty'] = final_rankings[-1].get('certainty_level', 'unknown')\n",
    "                \n",
    "                # Extract economic outcomes\n",
    "                outcomes = agent_info.get('economic_outcomes', [])\n",
    "                total_payout = sum(outcome.get('payout_amount', 0) for outcome in outcomes)\n",
    "                agent_record['total_payout'] = total_payout\n",
    "                agent_record['num_outcomes'] = len(outcomes)\n",
    "                \n",
    "                all_data.append(agent_record)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {result_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    print(f\"✅ Loaded {len(df)} agent records from {len(result_files)} experiments\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "results_df = load_experiment_data(results_dir)\n",
    "\n",
    "if not results_df.empty:\n",
    "    print(f\"\\n📊 Dataset Overview:\")\n",
    "    print(f\"   Total agent records: {len(results_df)}\")\n",
    "    print(f\"   Unique experiments: {results_df['experiment_id'].nunique()}\")\n",
    "    print(f\"   Models: {results_df['model'].unique()}\")\n",
    "    print(f\"   Temperatures: {sorted(results_df['temperature'].unique())}\")\n",
    "    print(f\"   Prompt types: {results_df['prompt_type'].unique()}\")\n",
    "    print(f\"   Consensus rate: {results_df['consensus_reached'].mean():.1%}\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\n🔍 Sample data:\")\n",
    "    print(results_df[['experiment_id', 'model', 'temperature', 'prompt_type', 'consensus_reached', 'agreed_principle']].head())\n",
    "else:\n",
    "    print(\"❌ No data loaded. Check if experiments completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Justice Criterion Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    # Overall distribution of justice criteria\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Final Justice Criterion Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Overall distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    experiment_consensus = results_df.groupby('experiment_id')['agreed_principle'].first()\n",
    "    principle_counts = experiment_consensus.value_counts()\n",
    "    \n",
    "    if not principle_counts.empty:\n",
    "        principle_counts.plot(kind='bar', ax=ax1, color='skyblue')\n",
    "        ax1.set_title('Distribution of Agreed Justice Principles')\n",
    "        ax1.set_ylabel('Number of Experiments')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(principle_counts.values):\n",
    "            ax1.text(i, v + 0.1, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Consensus achievement rate\n",
    "    ax2 = axes[0, 1]\n",
    "    consensus_rate = results_df.groupby('experiment_id')['consensus_reached'].first().value_counts()\n",
    "    consensus_rate.plot(kind='pie', ax=ax2, autopct='%1.1f%%', colors=['#ff7f7f', '#7fbf7f'])\n",
    "    ax2.set_title('Consensus Achievement Rate')\n",
    "    ax2.set_ylabel('')\n",
    "    \n",
    "    # 3. Principles by model\n",
    "    ax3 = axes[1, 0]\n",
    "    model_principles = results_df.groupby(['experiment_id', 'model'])['agreed_principle'].first().reset_index()\n",
    "    model_crosstab = pd.crosstab(model_principles['model'], model_principles['agreed_principle'])\n",
    "    \n",
    "    if not model_crosstab.empty:\n",
    "        model_crosstab.plot(kind='bar', ax=ax3, stacked=True)\n",
    "        ax3.set_title('Justice Principles by Model')\n",
    "        ax3.set_ylabel('Number of Experiments')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        ax3.legend(title='Principle', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 4. Principles by prompt type\n",
    "    ax4 = axes[1, 1]\n",
    "    prompt_principles = results_df.groupby(['experiment_id', 'prompt_type'])['agreed_principle'].first().reset_index()\n",
    "    prompt_crosstab = pd.crosstab(prompt_principles['prompt_type'], prompt_principles['agreed_principle'])\n",
    "    \n",
    "    if not prompt_crosstab.empty:\n",
    "        prompt_crosstab.plot(kind='bar', ax=ax4, stacked=True)\n",
    "        ax4.set_title('Justice Principles by Prompt Type')\n",
    "        ax4.set_ylabel('Number of Experiments')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        ax4.legend(title='Principle', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n📊 Justice Criterion Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not principle_counts.empty:\n",
    "        for principle, count in principle_counts.items():\n",
    "            percentage = (count / len(experiment_consensus)) * 100\n",
    "            print(f\"   {principle}: {count} experiments ({percentage:.1f}%)\")\n",
    "    \n",
    "    consensus_achieved = results_df.groupby('experiment_id')['consensus_reached'].first().sum()\n",
    "    total_experiments = results_df['experiment_id'].nunique()\n",
    "    print(f\"\\n   Consensus achieved: {consensus_achieved}/{total_experiments} experiments ({consensus_achieved/total_experiments:.1%})\")\n",
    "else:\n",
    "    print(\"⚠️ No data available for justice criterion analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Parameter Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Parameter Impact Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Prepare experiment-level data\n",
    "    exp_data = results_df.groupby('experiment_id').agg({\n",
    "        'model': 'first',\n",
    "        'temperature': 'first', \n",
    "        'prompt_type': 'first',\n",
    "        'consensus_reached': 'first',\n",
    "        'agreed_principle': 'first',\n",
    "        'total_duration': 'first',\n",
    "        'total_payout': 'mean'  # Average payout across agents\n",
    "    }).reset_index()\n",
    "    \n",
    "    # 1. Model impact on consensus\n",
    "    ax1 = axes[0, 0]\n",
    "    model_consensus = exp_data.groupby('model')['consensus_reached'].agg(['mean', 'count']).reset_index()\n",
    "    bars1 = ax1.bar(model_consensus['model'], model_consensus['mean'], \n",
    "                    color=['lightblue', 'lightcoral'])\n",
    "    ax1.set_title('Consensus Rate by Model')\n",
    "    ax1.set_ylabel('Consensus Rate')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels and counts\n",
    "    for i, (bar, rate, count) in enumerate(zip(bars1, model_consensus['mean'], model_consensus['count'])):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{rate:.1%}\\n(n={count})', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Temperature impact on consensus\n",
    "    ax2 = axes[0, 1]\n",
    "    temp_consensus = exp_data.groupby('temperature')['consensus_reached'].agg(['mean', 'count']).reset_index()\n",
    "    ax2.plot(temp_consensus['temperature'], temp_consensus['mean'], 'o-', linewidth=2, markersize=8)\n",
    "    ax2.set_title('Consensus Rate by Temperature')\n",
    "    ax2.set_xlabel('Temperature')\n",
    "    ax2.set_ylabel('Consensus Rate')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for temp, rate, count in zip(temp_consensus['temperature'], temp_consensus['mean'], temp_consensus['count']):\n",
    "        ax2.annotate(f'{rate:.1%}\\n(n={count})', (temp, rate), \n",
    "                    textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    # 3. Prompt type impact on consensus\n",
    "    ax3 = axes[1, 0]\n",
    "    prompt_consensus = exp_data.groupby('prompt_type')['consensus_reached'].agg(['mean', 'count']).reset_index()\n",
    "    bars3 = ax3.bar(prompt_consensus['prompt_type'], prompt_consensus['mean'],\n",
    "                    color=['lightgreen', 'lightyellow', 'lightpink'])\n",
    "    ax3.set_title('Consensus Rate by Prompt Type')\n",
    "    ax3.set_ylabel('Consensus Rate')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, rate, count in zip(bars3, prompt_consensus['mean'], prompt_consensus['count']):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{rate:.1%}\\n(n={count})', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Duration by parameters\n",
    "    ax4 = axes[1, 1]\n",
    "    duration_by_temp = exp_data.groupby(['temperature', 'consensus_reached'])['total_duration'].mean().unstack()\n",
    "    \n",
    "    if duration_by_temp.shape[1] > 1:\n",
    "        duration_by_temp.plot(kind='bar', ax=ax4, color=['red', 'green'])\n",
    "        ax4.set_title('Average Duration by Temperature & Consensus')\n",
    "        ax4.set_xlabel('Temperature')\n",
    "        ax4.set_ylabel('Duration (seconds)')\n",
    "        ax4.legend(['No Consensus', 'Consensus Reached'])\n",
    "        ax4.tick_params(axis='x', rotation=0)\n",
    "    else:\n",
    "        # Fallback if no variation in consensus\n",
    "        temp_duration = exp_data.groupby('temperature')['total_duration'].mean()\n",
    "        temp_duration.plot(kind='bar', ax=ax4, color='steelblue')\n",
    "        ax4.set_title('Average Duration by Temperature')\n",
    "        ax4.set_xlabel('Temperature')\n",
    "        ax4.set_ylabel('Duration (seconds)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(\"\\n📈 Parameter Impact Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\n🤖 Model Impact:\")\n",
    "    for _, row in model_consensus.iterrows():\n",
    "        print(f\"   {row['model']}: {row['mean']:.1%} consensus rate (n={row['count']})\")\n",
    "    \n",
    "    print(\"\\n🌡️ Temperature Impact:\")\n",
    "    for _, row in temp_consensus.iterrows():\n",
    "        print(f\"   Temperature {row['temperature']}: {row['mean']:.1%} consensus rate (n={row['count']})\")\n",
    "    \n",
    "    print(\"\\n💬 Prompt Type Impact:\")\n",
    "    for _, row in prompt_consensus.iterrows():\n",
    "        print(f\"   {row['prompt_type']}: {row['mean']:.1%} consensus rate (n={row['count']})\")\n",
    "else:\n",
    "    print(\"⚠️ No data available for parameter impact analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Satisfaction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    # Analyze preference rankings evolution\n",
    "    def analyze_preference_evolution(df):\n",
    "        \"\"\"\n",
    "        Analyze how agent preferences evolve through the experiment phases.\n",
    "        \"\"\"\n",
    "        # Filter to agents with complete ranking data\n",
    "        complete_agents = df.dropna(subset=['initial_ranking', 'final_ranking'])\n",
    "        \n",
    "        if complete_agents.empty:\n",
    "            print(\"⚠️ No complete preference ranking data available\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n📊 Preference Evolution Analysis (n={len(complete_agents)} agents)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Calculate preference stability\n",
    "        def ranking_stability(initial, final):\n",
    "            \"\"\"Calculate how much rankings changed between phases.\"\"\"\n",
    "            if not isinstance(initial, list) or not isinstance(final, list):\n",
    "                return None\n",
    "            if len(initial) != len(final):\n",
    "                return None\n",
    "            return sum(1 for i, f in zip(initial, final) if i == f) / len(initial)\n",
    "        \n",
    "        complete_agents['stability'] = complete_agents.apply(\n",
    "            lambda row: ranking_stability(row['initial_ranking'], row['final_ranking']), axis=1\n",
    "        )\n",
    "        \n",
    "        # Remove rows where stability couldn't be calculated\n",
    "        stable_agents = complete_agents.dropna(subset=['stability'])\n",
    "        \n",
    "        if not stable_agents.empty:\n",
    "            print(f\"\\n🎯 Preference Stability:\")\n",
    "            print(f\"   Mean stability: {stable_agents['stability'].mean():.1%}\")\n",
    "            print(f\"   Stability by temperature:\")\n",
    "            temp_stability = stable_agents.groupby('temperature')['stability'].mean()\n",
    "            for temp, stability in temp_stability.items():\n",
    "                print(f\"     Temperature {temp}: {stability:.1%}\")\n",
    "        \n",
    "        return stable_agents\n",
    "    \n",
    "    # Run preference evolution analysis\n",
    "    stable_data = analyze_preference_evolution(results_df)\n",
    "    \n",
    "    # Visualization of preference analysis\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Satisfaction and Preference Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Preference stability by temperature\n",
    "    ax1 = axes[0, 0]\n",
    "    if stable_data is not None and not stable_data.empty:\n",
    "        temp_stability = stable_data.groupby('temperature')['stability'].agg(['mean', 'std', 'count']).reset_index()\n",
    "        ax1.bar(temp_stability['temperature'], temp_stability['mean'], \n",
    "                yerr=temp_stability['std'], capsize=5, color='lightblue')\n",
    "        ax1.set_title('Preference Stability by Temperature')\n",
    "        ax1.set_xlabel('Temperature')\n",
    "        ax1.set_ylabel('Stability (proportion same rankings)')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels\n",
    "        for temp, mean, count in zip(temp_stability['temperature'], temp_stability['mean'], temp_stability['count']):\n",
    "            ax1.text(temp, mean + 0.05, f'{mean:.1%}\\n(n={count})', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Certainty levels distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    certainty_cols = ['initial_certainty', 'final_certainty']\n",
    "    certainty_data = []\n",
    "    \n",
    "    for col in certainty_cols:\n",
    "        if col in results_df.columns:\n",
    "            certainty_counts = results_df[col].value_counts()\n",
    "            for cert, count in certainty_counts.items():\n",
    "                certainty_data.append({'Phase': col.replace('_certainty', ''), 'Certainty': cert, 'Count': count})\n",
    "    \n",
    "    if certainty_data:\n",
    "        cert_df = pd.DataFrame(certainty_data)\n",
    "        cert_pivot = cert_df.pivot(index='Certainty', columns='Phase', values='Count').fillna(0)\n",
    "        cert_pivot.plot(kind='bar', ax=ax2, stacked=True)\n",
    "        ax2.set_title('Certainty Levels Distribution')\n",
    "        ax2.set_ylabel('Number of Agents')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        ax2.legend(title='Phase')\n",
    "    \n",
    "    # 3. Payout distribution by consensus\n",
    "    ax3 = axes[1, 0]\n",
    "    if 'total_payout' in results_df.columns:\n",
    "        consensus_groups = results_df.groupby('consensus_reached')['total_payout']\n",
    "        \n",
    "        # Box plot\n",
    "        box_data = [group.values for name, group in consensus_groups]\n",
    "        box_labels = [f\"{'Consensus' if name else 'No Consensus'}\\n(n={len(group)})\" \n",
    "                     for name, group in consensus_groups]\n",
    "        \n",
    "        ax3.boxplot(box_data, labels=box_labels)\n",
    "        ax3.set_title('Payout Distribution by Consensus')\n",
    "        ax3.set_ylabel('Total Payout ($)')\n",
    "    \n",
    "    # 4. Model vs Prompt interaction\n",
    "    ax4 = axes[1, 1]\n",
    "    model_prompt_consensus = results_df.groupby(['model', 'prompt_type'])['consensus_reached'].mean().unstack()\n",
    "    \n",
    "    if not model_prompt_consensus.empty:\n",
    "        sns.heatmap(model_prompt_consensus, annot=True, fmt='.1%', ax=ax4, cmap='RdYlGn')\n",
    "        ax4.set_title('Consensus Rate: Model × Prompt Type')\n",
    "        ax4.set_xlabel('Prompt Type')\n",
    "        ax4.set_ylabel('Model')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional satisfaction metrics\n",
    "    print(\"\\n💰 Economic Outcome Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if 'total_payout' in results_df.columns:\n",
    "        payout_stats = results_df.groupby('consensus_reached')['total_payout'].describe()\n",
    "        print(\"\\n📊 Payout Statistics by Consensus:\")\n",
    "        print(payout_stats.round(4))\n",
    "        \n",
    "        # Payout by parameters\n",
    "        print(\"\\n💎 Average Payout by Parameters:\")\n",
    "        temp_payout = results_df.groupby('temperature')['total_payout'].mean()\n",
    "        for temp, payout in temp_payout.items():\n",
    "            print(f\"   Temperature {temp}: ${payout:.4f}\")\n",
    "else:\n",
    "    print(\"⚠️ No data available for satisfaction analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comprehensive Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"📋 SYSTEMATIC ANALYSIS COMPREHENSIVE REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_experiments = results_df['experiment_id'].nunique()\n",
    "    total_agents = len(results_df)\n",
    "    consensus_rate = results_df.groupby('experiment_id')['consensus_reached'].first().mean()\n",
    "    \n",
    "    print(f\"\\n🎯 EXPERIMENT OVERVIEW:\")\n",
    "    print(f\"   Total experiments completed: {total_experiments}\")\n",
    "    print(f\"   Total agent records: {total_agents}\")\n",
    "    print(f\"   Overall consensus rate: {consensus_rate:.1%}\")\n",
    "    print(f\"   Average experiment duration: {results_df['total_duration'].mean():.1f} seconds\")\n",
    "    \n",
    "    # Parameter breakdown\n",
    "    print(f\"\\n🔬 PARAMETER COVERAGE:\")\n",
    "    print(f\"   Models tested: {', '.join(results_df['model'].unique())}\")\n",
    "    print(f\"   Temperatures tested: {sorted(results_df['temperature'].unique())}\")\n",
    "    print(f\"   Prompt types tested: {', '.join(results_df['prompt_type'].unique())}\")\n",
    "    \n",
    "    # Key findings\n",
    "    print(f\"\\n🏆 KEY FINDINGS:\")\n",
    "    \n",
    "    # Best performing configurations\n",
    "    exp_summary = results_df.groupby(['model', 'temperature', 'prompt_type']).agg({\n",
    "        'consensus_reached': 'mean',\n",
    "        'experiment_id': 'count',\n",
    "        'total_duration': 'mean'\n",
    "    }).round(3).reset_index()\n",
    "    \n",
    "    best_consensus = exp_summary.loc[exp_summary['consensus_reached'].idxmax()]\n",
    "    print(f\"   Best consensus rate: {best_consensus['consensus_reached']:.1%}\")\n",
    "    print(f\"     Model: {best_consensus['model']}\")\n",
    "    print(f\"     Temperature: {best_consensus['temperature']}\")\n",
    "    print(f\"     Prompt: {best_consensus['prompt_type']}\")\n",
    "    \n",
    "    # Most common principle\n",
    "    principle_counts = results_df.groupby('experiment_id')['agreed_principle'].first().value_counts()\n",
    "    if not principle_counts.empty:\n",
    "        most_common = principle_counts.index[0]\n",
    "        most_common_count = principle_counts.iloc[0]\n",
    "        print(f\"\\n   Most agreed upon principle: {most_common}\")\n",
    "        print(f\"     Chosen in {most_common_count}/{total_experiments} experiments ({most_common_count/total_experiments:.1%})\")\n",
    "    \n",
    "    # Temperature effects\n",
    "    temp_effects = results_df.groupby('temperature')['consensus_reached'].mean()\n",
    "    best_temp = temp_effects.idxmax()\n",
    "    print(f\"\\n   Most effective temperature: {best_temp} ({temp_effects[best_temp]:.1%} consensus rate)\")\n",
    "    \n",
    "    # Model comparison\n",
    "    model_comparison = results_df.groupby('model')['consensus_reached'].mean()\n",
    "    print(f\"\\n   Model performance:\")\n",
    "    for model, rate in model_comparison.items():\n",
    "        print(f\"     {model}: {rate:.1%} consensus rate\")\n",
    "    \n",
    "    # Prompt type effects\n",
    "    prompt_effects = results_df.groupby('prompt_type')['consensus_reached'].mean()\n",
    "    best_prompt = prompt_effects.idxmax()\n",
    "    print(f\"\\n   Most effective prompt type: {best_prompt} ({prompt_effects[best_prompt]:.1%} consensus rate)\")\n",
    "    \n",
    "    print(f\"\\n📊 DATA EXPORT:\")\n",
    "    \n",
    "    # Save summary data\n",
    "    summary_file = systematic_dir / \"analysis_summary.csv\"\n",
    "    results_df.to_csv(summary_file, index=False)\n",
    "    print(f\"   Full dataset saved to: {summary_file}\")\n",
    "    \n",
    "    # Save experiment-level summary\n",
    "    exp_level_summary = results_df.groupby('experiment_id').agg({\n",
    "        'model': 'first',\n",
    "        'temperature': 'first',\n",
    "        'prompt_type': 'first',\n",
    "        'consensus_reached': 'first',\n",
    "        'agreed_principle': 'first',\n",
    "        'total_duration': 'first',\n",
    "        'total_payout': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    exp_summary_file = systematic_dir / \"experiment_summary.csv\"\n",
    "    exp_level_summary.to_csv(exp_summary_file, index=False)\n",
    "    print(f\"   Experiment summary saved to: {exp_summary_file}\")\n",
    "    \n",
    "    print(f\"\\n✅ SYSTEMATIC ANALYSIS COMPLETE\")\n",
    "    print(f\"   Results available in: {systematic_dir}\")\n",
    "    print(f\"   Total execution time: {results_df['total_duration'].sum():.0f} seconds\")\n",
    "    print(f\"   Average per experiment: {results_df['total_duration'].mean():.1f} seconds\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No data available for comprehensive report\")\n",
    "    print(\"💡 Make sure experiments completed successfully before running analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps and Configuration Options\n",
    "\n",
    "### To Run Full Analysis:\n",
    "1. Set `TEST_MODE = False` in the execution section\n",
    "2. Increase `MAX_CONCURRENT` if your system can handle more parallel experiments\n",
    "3. Add GPT-4.1 to the MODELS list for complete coverage\n",
    "\n",
    "### To Modify Parameters:\n",
    "- **Models**: Edit the `MODELS` list\n",
    "- **Temperatures**: Modify the `TEMPERATURES` list  \n",
    "- **Prompts**: Update the `PROMPTS` dictionary\n",
    "- **Distributions**: Modify `INCOME_DISTRIBUTIONS` (currently matches new_logic.md)\n",
    "\n",
    "### Analysis Extensions:\n",
    "- Add more detailed preference evolution tracking\n",
    "- Include individual agent conversation analysis\n",
    "- Add statistical significance testing\n",
    "- Export results for external analysis tools\n",
    "\n",
    "This notebook provides a complete framework for systematic analysis of multi-agent distributive justice experiments with parallel execution and comprehensive visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Running specific configuration: configs/lucas_new.yaml\n",
      "Loaded 3 agents:\n",
      "  - Agent_1_us_student: gpt-4.1 (custom personality)\n",
      "  - Agent_2_us_student: gpt-4.1 (custom personality)\n",
      "  - Agent_3_us_student: gpt-4.1 (custom personality)\n",
      "\n",
      "=== Starting New Game Logic Experiment ===\n",
      "Experiment ID: systematic_001_gpt_4.1_0_us_student\n",
      "Agents: 3\n",
      "Individual Rounds: 4 (fixed per new_logic.md)\n",
      "Group Deliberation Max Rounds: 6\n",
      "Income Distributions: 4\n",
      "Payout Ratio: $0.0001 per $1\n",
      "\n",
      "--- Initializing Agents ---\n",
      "Created 3 deliberation agents\n",
      "  - Agent_1_us_student (agent_1)\n",
      "  - Agent_2_us_student (agent_2)\n",
      "  - Agent_3_us_student (agent_3)\n",
      "\n",
      "=== PHASE 1: Individual Familiarization ===\n",
      "\n",
      "--- Phase 1.1: Initial Preference Ranking ---\n",
      "Collected initial rankings from 3 agents\n",
      "\n",
      "--- Phase 1.2: Detailed Examples ---\n",
      "Presenting detailed examples of principle outcomes to agents...\n",
      "  Agent_1_us_student: RunResult:\n",
      "- Last agent: Agent(name=\"Agent_1_us_student\", ...)\n",
      "- Final output (str):\n",
      "    Certainly! Here’s my summary of what I learned from the examples and mappings:\n",
      "    \n",
      "    - Each distributive justice principle corresponds to a specific income distribution among the available options.\n",
      "    - **Maximizing the floor** (helping the worst-off as much as possible) always selects Distribution 4.\n",
      "    - **Maximizing the average** (maximizing total/average income) always selects Distribution 1.\n",
      "    - **Maximizing the average with a floor constraint** selects:\n",
      "      - Distribution 1 if the minimum income (floor) is set at $12,000 or less,\n",
      "      - Distribution 3 if the floor is set at $13,000 or $14,000,\n",
      "      - Distribution 4 if the floor is set at $15,000.\n",
      "    - **Maximizing the average with a range constraint** selects:\n",
      "      - Distribution 1 if the maximum allowed income gap (range) is $20,000 or more,\n",
      "      - Distribution 3 if the range is $17,000,\n",
      "      - Distribution 2 if the range is $15,000.\n",
      "    \n",
      "    This means that the specific distribution chosen depends not only on the principle but also on the value set for the floor or range constraint. Understanding these mappings helps me see how each principle would affect income outcomes for everyone in the society.\n",
      "- 1 new item(s)\n",
      "- 1 raw response(s)\n",
      "- 0 input guardrail result(s)\n",
      "- 0 output guardrail result(s)\n",
      "(See `RunResult` for more details)\n",
      "  Agent_2_us_student: RunResult:\n",
      "- Last agent: Agent(name=\"Agent_2_us_student\", ...)\n",
      "- Final output (str):\n",
      "    Certainly! Here’s my summary of what I learned from the examples and mappings:\n",
      "    \n",
      "    - Each distributive justice principle corresponds to a specific income distribution among the available options.\n",
      "    - **Maximizing the floor** (helping the worst-off as much as possible) always selects Distribution 4.\n",
      "    - **Maximizing the average** (maximizing total/average income) always selects Distribution 1.\n",
      "    - **Maximizing the average with a floor constraint** selects:\n",
      "      - Distribution 1 if the minimum income floor is set at $12,000 or less,\n",
      "      - Distribution 3 if the floor is set at $13,000 or $14,000,\n",
      "      - Distribution 4 if the floor is set at $15,000.\n",
      "    - **Maximizing the average with a range constraint** selects:\n",
      "      - Distribution 1 if the maximum allowed income range is $20,000 or more,\n",
      "      - Distribution 3 if the range is $17,000,\n",
      "      - Distribution 2 if the range is $15,000.\n",
      "    \n",
      "    This means that the specific constraint values for principles 3 and 4 determine which distribution is chosen, and each principle leads to a different pattern of income distribution across society. This helps me understand the real impact of each principle on both the poorest and the overall average income.\n",
      "- 1 new item(s)\n",
      "- 1 raw response(s)\n",
      "- 0 input guardrail result(s)\n",
      "- 0 output guardrail result(s)\n",
      "(See `RunResult` for more details)\n",
      "  Agent_3_us_student: RunResult:\n",
      "- Last agent: Agent(name=\"Agent_3_us_student\", ...)\n",
      "- Final output (str):\n",
      "    Certainly! Here’s my summary of what I learned from the examples and mappings:\n",
      "    \n",
      "    - Each distributive justice principle corresponds to a specific income distribution among the available options.\n",
      "    - **Maximizing the floor** always selects Distribution 4, which gives the highest possible minimum income to the poorest person.\n",
      "    - **Maximizing the average** always selects Distribution 1, which gives the highest possible average income, regardless of how it’s distributed.\n",
      "    - **Maximizing the average with a floor constraint** selects different distributions depending on the minimum income guaranteed:\n",
      "      - If the floor is set at $12,000 or less, it picks Distribution 1.\n",
      "      - If the floor is set at $13,000 or $14,000, it picks Distribution 3.\n",
      "      - If the floor is set at $15,000, it picks Distribution 4.\n",
      "    - **Maximizing the average with a range constraint** also selects different distributions depending on the maximum allowed income gap:\n",
      "      - If the range is $20,000 or more, it picks Distribution 1.\n",
      "      - If the range is $17,000, it picks Distribution 3.\n",
      "      - If the range is $15,000, it picks Distribution 2.\n",
      "    \n",
      "    This means the specific distribution chosen depends on both the principle and the value of any constraints (floor or range). Understanding these mappings helps me see how each principle would affect income outcomes for everyone in the group.\n",
      "- 1 new item(s)\n",
      "- 1 raw response(s)\n",
      "- 0 input guardrail result(s)\n",
      "- 0 output guardrail result(s)\n",
      "(See `RunResult` for more details)\n",
      "Detailed examples presented to 3 agents\n",
      "\n",
      "--- Phase 1.2.1: Second Assessment (After Detailed Examples) ---\n",
      "Collected post-examples rankings from 3 agents\n",
      "\n",
      "--- Phase 1.3: Individual Application Rounds (4 rounds) ---\n",
      "\n",
      "  Round 1/4\n",
      "    Agent_1_us_student: Principle 3 -> Low class ($12,000, payout: $1.20)\n",
      "    Agent_2_us_student: Principle 3 -> Medium low class ($13,000, payout: $1.30)\n",
      "    Agent_3_us_student: Principle 3 -> Medium high class ($27,000, payout: $2.70)\n",
      "\n",
      "  Round 2/4\n",
      "    Agent_1_us_student: Principle 3 -> High class ($32,000, payout: $3.20)\n",
      "    Agent_2_us_student: Principle 3 -> Medium class ($24,000, payout: $2.40)\n",
      "    Agent_3_us_student: Principle 3 -> High class ($32,000, payout: $3.20)\n",
      "\n",
      "  Round 3/4\n",
      "    Agent_1_us_student: Principle 3 -> Medium high class ($27,000, payout: $2.70)\n",
      "    Agent_2_us_student: Principle 3 -> Medium low class ($13,000, payout: $1.30)\n",
      "    Agent_3_us_student: Principle 3 -> Medium class ($24,000, payout: $2.40)\n",
      "\n",
      "  Round 4/4\n",
      "    Agent_1_us_student: Principle 3 -> Medium class ($24,000, payout: $2.40)\n",
      "    Agent_2_us_student: Principle 3 -> Low class ($12,000, payout: $1.20)\n",
      "    Agent_3_us_student: Principle 3 -> High class ($32,000, payout: $3.20)\n",
      "Completed 4 individual rounds with 12 total outcomes\n",
      "\n",
      "--- Phase 1.3.1: Third Assessment (After Individual Application) ---\n",
      "Collected post-individual rankings from 3 agents\n",
      "\n",
      "=== PHASE 2: Group Experiment ===\n",
      "\n",
      "--- Phase 2.1: Group Deliberation (max 6 rounds) ---\n",
      "\n",
      "--- Starting Multi-Round Deliberation ---\n",
      "\n",
      "--- Round 1 ---\n",
      "  Speaking order: ['Agent_1_us_student', 'Agent_2_us_student', 'Agent_3_us_student']\n",
      "    Agent_3_us_student (Position 1)\n",
      "      Chose Principle 3\n",
      "      Strategy: One specific thing I could do in the next round to move toward consensus is:\n",
      "\n",
      "**Argument/Point:** Emphasize the uncertainty of our future income class and how that makes fairness especially important.\n",
      "\n",
      "**Target Agent:** Address the whole group, but especially anyone expressing concern about maximizing their own potential income (i.e., those leaning toward maximizing the average).\n",
      "\n",
      "**Concern Addressed:** The risk that any of us could end up in the lowest income class, and the need for a safety net.\n",
      "\n",
      "**Framing:**  \n",
      "\"I want to remind everyone that since our income class will be assigned randomly after we choose the principle, any of us could end up at the bottom. If we pick a principle that only maximizes the average, some of us could get stuck with very little. But if we choose a principle that guarantees a minimum income for everyone—like maximizing the floor or setting a floor constraint—we make sure no one is left behind, no matter where they end up. I think that's the fairest approach given our uncertainty.\"\n",
      "    Agent_2_us_student (Position 2)\n",
      "      Chose Principle 3\n",
      "      Strategy: **One specific thing I could do in the next round to move toward consensus:**\n",
      "\n",
      "**Strategy:**  \n",
      "Directly address Agent_3_us_student’s concern about uncertainty and the need for a safety net by proposing a concrete, reasonable value for the minimum income floor in Principle 3, and invite their input on what that value should be.\n",
      "\n",
      "**How to do it:**  \n",
      "Say:  \n",
      "*\"Agent_3_us_student, since you emphasized the importance of a guaranteed minimum income because we don’t know our future income class, I agree that Principle 3 is a strong option. To move forward, could we discuss what a fair minimum income floor would be? If we can agree on a specific number, I think more of us could get on board with Principle 3 and reach consensus.\"*\n",
      "\n",
      "**Why this works:**  \n",
      "- It shows you are listening to and validating their main concern.\n",
      "- It moves the discussion from abstract agreement to concrete action.\n",
      "- It invites collaboration and signals willingness to compromise, which can help build consensus.\n",
      "    Agent_1_us_student (Position 3)\n",
      "      Chose Principle 3\n",
      "      Strategy: **One specific thing I could do in the next round to move toward consensus:**\n",
      "\n",
      "**Strategy:**  \n",
      "Directly address Agent_2_us_student’s call for group input by proposing a concrete minimum income floor value, and invite feedback to show responsiveness to their priority for inclusive decision-making.\n",
      "\n",
      "**How to do it:**  \n",
      "Say:  \n",
      "“I agree with Agent_2_us_student that it’s important to set a fair minimum income floor. I propose we set the floor at $X (choose a reasonable, concrete number—e.g., $30 if the range is $0–$100). Does this amount feel secure and fair to everyone? If not, what would you suggest? Let’s try to agree on a specific number so we can move forward together.”\n",
      "\n",
      "**Why this works:**  \n",
      "- **Focuses on Agent_2_us_student,** who wants group discussion and a specific floor value.\n",
      "- **Addresses their concern** for fairness and security by proposing a concrete number.\n",
      "- **Frames the message** as collaborative and solution-oriented, inviting others to adjust or agree, which helps build consensus.\n",
      "  Round 1 completed in 55.8s\n",
      "  ✓ Unanimous agreement reached!\n",
      "\n",
      "--- Phase 2.3: Group Economic Outcomes ---\n",
      "Applying agreed principle: MAXIMIZING THE AVERAGE WITH A FLOOR CONSTRAINT\n",
      "  Agent_1_us_student: Medium class ($19,000, payout: $1.90)\n",
      "  Agent_2_us_student: Medium class ($19,000, payout: $1.90)\n",
      "  Agent_3_us_student: Low class ($15,000, payout: $1.50)\n",
      "\n",
      "--- Phase 2.4: Final Preference Ranking ---\n",
      "Collected final rankings from 3 agents\n",
      "\n",
      "=== Experiment Complete ===\n",
      "Consensus reached: True\n",
      "Agreed principle: MAXIMIZING THE AVERAGE WITH A FLOOR CONSTRAINT\n",
      "Individual rounds completed: 4\n",
      "Total economic outcomes: 15\n",
      "\n",
      "--- Data Export Complete ---\n",
      "  Unified Agent-Centric JSON: systematic_analysis/specific_results/systematic_001_gpt_4.1_0_us_student.json\n",
      "✅ SUCCESS: systematic_001_gpt_4.1_0_us_student\n",
      "   Output saved to: systematic_analysis/specific_results/systematic_001_gpt_4.1_0_us_student.json\n",
      "   Duration: 154.8 seconds\n",
      "\\n📊 Quick Results:\n",
      "   Consensus reached: True\n",
      "   Agreed principle: MAXIMIZING THE AVERAGE WITH A FLOOR CONSTRAINT\n",
      "   Total duration: 154.8s\n",
      "   Agents: 3\n"
     ]
    }
   ],
   "source": [
    "# Run specific configuration: configs/lucas_new.yaml\n",
    "print(\"🧪 Running specific configuration: configs/lucas_new.yaml\")\n",
    "\n",
    "async def run_specific_config():\n",
    "    \"\"\"Run the lucas_new.yaml configuration specifically.\"\"\"\n",
    "    \n",
    "    # Run single experiment\n",
    "    from maai.runners import run_experiment\n",
    "    \n",
    "    try:\n",
    "        result = await run_experiment(\n",
    "            'lucas_new',  # Configuration name (without .yaml extension)\n",
    "            output_dir='systematic_analysis/specific_results',  # Custom output directory\n",
    "            config_dir='configs'  # Standard configs directory\n",
    "        )\n",
    "        \n",
    "        if result.get('success', False):\n",
    "            print(f\"✅ SUCCESS: {result['experiment_id']}\")\n",
    "            print(f\"   Output saved to: {result['output_path']}\")\n",
    "            print(f\"   Duration: {result.get('duration_seconds', 0):.1f} seconds\")\n",
    "            \n",
    "            # Load and display basic results\n",
    "            try:\n",
    "                import json\n",
    "                with open(result['output_path'], 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                metadata = data.get('experiment_metadata', {})\n",
    "                consensus = metadata.get('final_consensus', {})\n",
    "                \n",
    "                print(f\"\\\\n📊 Quick Results:\")\n",
    "                print(f\"   Consensus reached: {consensus.get('agreement_reached', 'Unknown')}\")\n",
    "                if consensus.get('agreed_principle'):\n",
    "                    print(f\"   Agreed principle: {consensus.get('agreed_principle')}\")\n",
    "                print(f\"   Total duration: {metadata.get('total_duration_seconds', 0):.1f}s\")\n",
    "                \n",
    "                # Count agents\n",
    "                agent_count = len([k for k in data.keys() if k != 'experiment_metadata'])\n",
    "                print(f\"   Agents: {agent_count}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Could not load detailed results: {e}\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"❌ FAILED: {result.get('error', 'Unknown error')}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error running experiment: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the specific configuration\n",
    "await run_specific_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Run Specific Configuration\n",
    "\n",
    "Run a specific existing configuration file for testing or individual analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
